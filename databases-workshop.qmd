---
format:
  pdf:
    number-sections: true
    # highlight-style: pygments
    highlight-style: files/mcshanepdf.theme
    fontsize: 11pt
    fig-pos: 'H'
    block-headings: false
    colorlinks: true
    shift-heading-level-by: 0
    geometry: 
      - top=1in
      - bottom=1in
      - left=1in
      - right=1in
    include-in-header:
      text: |
        \usepackage{fancyhdr, bm}
        \addtokomafont{disposition}{\rmfamily}
        \setcounter{section}{1}
        \lhead{ STAT2/37815 \\ Dr. McShane }
        \rhead{ YOUR NAME HERE \\ Due May 9th at 5pm }
        \chead{\textbf{\Large Homework \# 4}}
        \definecolor{codegray}{HTML}{f9f9f9}
        \definecolor{codeletter}{HTML}{002c6b}
        \let\textttOrig\texttt
        \renewcommand{\texttt}[1]{\textttOrig{\textbf{\textcolor{codeletter}{\colorbox{codegray}{#1}}}}}
        \definecolor{indigo}{RGB}{75, 0, 130}
        \definecolor{darkgreen}{RGB}{0, 128, 0}
crossref: 
  sec-prefix: Problem   # default is Section
---

\thispagestyle{fancy}

```{r setup, include = FALSE}
# Load necessary packages
# library(kableExtra)
library(formatR)

# Quarto options
knitr::opts_chunk$set(
  tidy = FALSE,     # display code as typed
  size = "small",    # slightly smaller font for code
  fig.width = 4,
  fig.height = 2.5,
  fig.align = "center", 
  cache = TRUE
)
ggplot2::theme_set(ggplot2::theme_bw())
 
# Makes kable table nice just by adding `|> kable()` to a table
# Use `escape = FALSE` to use LaTeX inside of a table
# kable = function(x, booktabs = TRUE, align = "c", format, digits = getOption("digits"), row.names = NA, col.names = NA, caption = NULL, label = NULL, format.args = list(), escape = TRUE, full_width = NULL, bootstrap_options = "basic", position = "center", latex_options = c("HOLD_position", "repeat_headers"), font_size = NULL, row_label_position = "l", ...) {
#   knitr::kable(x, booktabs = booktabs, align = align, format = format, digits = digits, row.names = row.names, col.names = col.names, caption = caption, label = label, format.args = format.args, escape = escape, ...) |>
#     kableExtra::kable_styling(full_width = full_width, bootstrap_options = bootstrap_options, position = position, latex_options = latex_options, font_size = font_size, row_label_position = row_label_position, ...)
# }
```

```{r DrM_Funks, echo = FALSE}
# Dr. McShane's functions
# ...
```


\newcommand{\ansbegin}{ \color{blue}\smallbreak\vspace{-8pt}\hrulefill \smallbreak\noindent}
\newcommand{\ansend}{\smallbreak\vspace{-8pt}\hrulefill \smallbreak\vspace{-8pt} \color{black} }

<!-- \Large \textbf{FOR THE ENTIRETY OF THIS ASSIGNMENT, DO NOT LOAD A PACKAGE.}  \normalsize _These are the only packages you should need to get started:_. -->

```{r libs, message = FALSE}
library(tidyverse)
library(dm)
library(DiagrammeR)
library(RSQLite)
library(RMariaDB)
library(duckdb)
library(duckplyr)
library(progress)
library(pixarfilms)
library(nycflights13)
library(parquetize)
```


\dotfill


<!-- NEW PROBLEM ----------------------------------------------->


## Question 10

## Setup

```{r }
# attach relevant packages
library(DBI)

### First steps ################################################################

# Connection -------------------------------------------------------------------

con <- dbConnect(duckdb::duckdb())
con

# Discover tables --------------------------------------------------------------

dbListTables(con)

# Populate database (normally done by other people) ---------------------------

# Magic: import tables into the database
dm::copy_dm_to(
  con,
  dm::dm_pixarfilms(),
  set_key_constraints = FALSE,
  temporary = FALSE
)

# Discover tables --------------------------------------------------------------

dbListTables(con)
dbListFields(con, "box_office")
```


## Exercises

```{r }
con
```

### 1. List all columns from the `pixar_films` table.

```{r}
dbListFields(con, "pixar_films")
```


### 2. Review the help for `dbListFields()` and `dbListTables()` and the index on <https://dbi.r-dbi.org/reference/>.

```{r}
help("dbListFields")
help("dbListTables")
browseURL("https://dbi.r-dbi.org/reference/")
```



## Question 11

### 

```{r }
# Discover tables --------------------------------------------------------------

dbListTables(con)
dbListFields(con, "pixar_films")
dbListFields(con, "academy")

# Read table -------------------------------------------------------------------

df_pixar_films <- dbReadTable(con, "pixar_films")
df_pixar_films
as_tibble(df_pixar_films)

# Execute queries --------------------------------------------------------------

dbGetQuery(con, "SELECT * FROM pixar_films")

# Assign SQL queries to character strings
sql <- "SELECT * FROM pixar_films WHERE release_date >= '2020-01-01'"

# new in R 4.1: r"()" syntax
# Kirill has used "" to indicate column names and '' for character strings
# sql <- r"(SELECT * FROM "pixar_films" WHERE "release_date" >= '2020-01-01')"
dbGetQuery(con, sql)

# Further pointers -------------------------------------------------------------

# Quoting identifiers
dbQuoteIdentifier(con, "academy")
dbQuoteIdentifier(con, "from")

# Quoting literals
dbQuoteLiteral(con, "Toy Story")
dbQuoteLiteral(con, as.Date("2020-01-01"))

# Paste queries with glue_sql()

# Parameterized queries
sql <- "SELECT count(*) FROM pixar_films WHERE release_date >= ?"
dbGetQuery(con, sql, params = list(as.Date("2020-01-01")))


# Incomplete sql query
# sql <- paste0(
#   "SELECT * FROM",
#   dbQuoteIdentifier(con, "academy"), 
#   " ", 
#   "pixar_films WHERE release_date >= ?"
# )
# 
# dbGetQuery(
#   con, 
#   sql, 
#   params = list(
#     c("Won", "Won"), 
#     c("Animated Feature", "Original Song")
#   )
# )

```


```{r }
# Reading tables: Exercises ----------------------------------------------------

con
# 1. Read the `academy` table.
# 2. Read all records from the `academy` table that correspond to awards won
#     - Hint: Use the query "SELECT * FROM academy WHERE status = 'Won'"
# 3. Use quoting and/or a query parameter to make the previous query more robust.
#     - Hint: `sql <- paste0("SELECT * FROM academy WHERE ", quoted_column, " = ?")`




```



<!-- NEW PROBLEM ----------------------------------------------->

## Question 12

## Setup

```{r}
### Downsizing on the database #################################################

# Connection -------------------------------------------------------------------

con <- DBI::dbConnect(duckdb::duckdb())
dm::copy_dm_to(con, dm::dm_pixarfilms(), set_key_constraints = FALSE, temporary = FALSE)

# Lazy tables ------------------------------------------------------------------

pixar_films <- tbl(con, "pixar_films")
pixar_films

# Get all data ----

df_pixar_films <-
  pixar_films |>
  collect()
df_pixar_films

# Get first 10 rows
pixar_films |>
  collect(n = 10)

# Get first 10 rows
pixar_films |>
  slice_sample(n = 10)

# Why does this work? Show_query helps
pixar_films |> 
  head() |>
  show_query()

# setting a seed in R session has no effect on database. 
# Thus, we will need to set a seed in the database
dbExecute(con, "SELECT setseed(.42)")

pixar_films |>
  slice_sample(n = 10) |> 
  show_query()
```


```{r}
# Projection (column selection)  -----------------------------------------------

pixar_films |>
  select(1:3)

# Computations happens on the database!
pixar_films |>
  select(1:3) |>
  show_query()

# Bring the data into the R session
df_pixar_films_3 <-
  pixar_films |>
  select(1:3) |>
  collect()
df_pixar_films_3

# Immutable data: original data unchanged
pixar_films |>
  collect()

# regex can work 
pixar_films |> 
  filter(grepl("^Toy ", film)) |> 
  collect()

# Hypothetically, if it didn't, just modify the data frame in R
pixar_films |> 
  collect() |>
  filter(grepl("^Toy ", film))

```


```{r}
# Filtering (row selection)  ---------------------------------------------------

pixar_films |>
  filter(release_date >= "2020-01-01")

# Computations happens on the database!
pixar_films |>
  filter(release_date >= "2020-01-01") |>
  show_query()

# Bring the data into the R session
df_pixar_films_202x <-
  pixar_films |>
  filter(release_date >= "2020-01-01") |>
  collect()
df_pixar_films_202x

# Immutable data: original data unchanged
pixar_films |>
  collect()
```

### Exercises 

```{r}
# Downsizing on the database: Exercises ----------------------------------------

# `select()` -------------------------------------------------------------------

pixar_films

# *  Find several ways to select the 3 first columns
## base R
pixar_films |>
  collect() %>%
  .[, 1:3]
## dplyr
pixar_films |> 
  select(1:3)
## dplyr ugly
pixar_films |> 
  select(!4:ncol(pixar_films))
## dplyr need to know column names
pixar_films |> 
  select(number:release_date)


# *  What happens if you include the name of a variable multiple times in a `select()` call?
pixar_films |> 
  select(number, number)

# *  Select all columns that contain underscores (use `contains()`)
pixar_films |> 
  select(contains("_"))

# *  Use `all_of()` to select 2 columns of your choice
columns_of_interest = pixar_films |> colnames() |> head(n = 2)
pixar_films |> 
  select(columns_of_interest)
pixar_films |> 
  select(all_of(columns_of_interest))
pixar_films |> 
  select(!!columns_of_interest)


# `filter()` -------------------------------------------------------------------

pixar_films

# Find all films that
# 1. Are rated "PG"

filter(pixar_films, film_rating == "PG")

# 2. Had a run time below 95

filter(pixar_films, run_time < 95)

# 3. Had a rating of "N/A" or "Not Rated"

filter(pixar_films, film_rating %in% c("N/A", "Not Rated"))

# 4. Were released after and including year 2020

filter(pixar_films, release_date >= as.Date("2020-01-01"))

# 5. Have a missing name (`film` column) or `run_time`

filter(pixar_films, is.na(film) | is.na(run_time))

# 6. Are a first sequel (the name ends with "2", as in "Toy Story 2")
#     - Hint: Bring the data into the R session before filtering

filter(collect(pixar_films), grepl("2$", film))

# `count()`, `summarize()`, `group_by()`, `ungroup()` --------------------------

pixar_films

# 1. How many films are stored in the table?

count(pixar_films)

# 2. How many films released after 2005 are stored in the table?

filter(pixar_films, release_date >= as.Date("2006-01-01")) |>
  count()

# 3. What is the total run time of all films?
#     - Hint: Use `summarize(sum(...))`, watch out for the warning

summarize(pixar_films, total_time = sum(run_time, na.rm = TRUE))

# 4. What is the total run time of all films, per rating?
#     - Hint: Use `group_by()` or `.by`

pixar_films |>
  summarize(.by = film_rating, total_time = sum(run_time, na.rm = TRUE))


```


## Question 13

```{r}
### Downsizing on the database #################################################

# Connection -------------------------------------------------------------------

con <- DBI::dbConnect(duckdb::duckdb())
dm::copy_dm_to(
  con, 
  dm::dm_pixarfilms(), 
  set_key_constraints = FALSE, 
  temporary = FALSE
)

# Lazy tables ------------------------------------------------------------------

pixar_films <- tbl(con, "pixar_films")
pixar_films
```


```{r}
# Aggregation ------------------------------------------------------------------

pixar_films |>
  summarize(
    .by = film_rating, 
    n = n()
  )

# Shortcut
pixar_films |>
  count(film_rating)

# Computations happens on the database!
pixar_films |>
  count(film_rating) |>
  show_query()

# Bring the data into the R session
df_pixar_films_by_rating <-
  pixar_films |>
  count(film_rating) |>
  collect()
df_pixar_films_by_rating

# Immutable data: original data unchanged
pixar_films |>
  collect()
```


```{r}
# Second lazy table --------------------------------------------------------------

academy <- tbl(con, "academy")

academy
academy |>
  count(status)
```


```{r}
# Left join ------

academy |>
  left_join(pixar_films)

academy |>
  left_join(pixar_films, join_by(film))

academy |>
  left_join(pixar_films, join_by(film)) |>
  show_query()
```


```{r}
# Join with prior computation ------

academy_won <-
  academy |>
  filter(status == "Won") |>
  count(film, name = "n_won")
academy_won

pixar_films |>
  left_join(academy_won, join_by(film))

academy_won |>
  right_join(pixar_films, join_by(film)) |>
  arrange(release_date)

academy_won |>
  right_join(pixar_films, join_by(film)) |>
  mutate(n_won = coalesce(n_won, 0L)) |>
  arrange(release_date)


# important point: this SQL statement is not necessarily what we would want to
# write by hand; if putting into production, would want to simplify SQL
pixar_films |>
  left_join(academy_won, join_by(film)) |>
  mutate(n_won = coalesce(n_won, 0L)) |>
  arrange(release_date) |>
  show_query()
```


```{r}
# Caveat: tables must be on the same source ------------------------------------

try(
  academy |>
    left_join(pixarfilms::pixar_films, join_by(film))
)

academy |>
  left_join(pixarfilms::pixar_films, join_by(film), copy = TRUE)

academy |>
  left_join(pixarfilms::pixar_films, join_by(film), copy = TRUE) |>
  show_query()

try(
  pixarfilms::academy |>
    left_join(pixar_films, join_by(film))
)

pixarfilms::academy |>
  left_join(pixar_films, join_by(film), copy = TRUE)

pixar_films_db <-
  copy_to(con, pixarfilms::pixar_films)

academy |>
  left_join(pixar_films_db, join_by(film))
```


```{r}
# Downsizing on the database: Exercises ----------------------------------------

# `count()`, `summarize()`, `group_by()`, `ungroup()` --------------------------

pixar_films

# 1. How many films are stored in the table?

pixar_films |> 
  count()

# 2. How many films released after 2005 are stored in the table?
## their solution
pixar_films |> 
  filter(release_date >= as.Date("2006-01-01"))
## better solution
pixar_films |> 
  filter(year(release_date) > 2005)

# 3. What is the total run time of all films?
#     - Hint: Use `summarize(sum(...))`, watch out for the warning

pixar_films |> 
  summarize(total_run_time = sum(run_time))


# 4. What is the total run time of all films, per rating?
#     - Hint: Use `group_by()` or `.by`

pixar_films |> 
  summarize(total_run_time = sum(run_time), .by = film_rating)
```


```{r}
# `left_join()` --------------------------------------------------------------------

pixar_films |>
  left_join(academy, join_by(film))

# 1. How many rows does the join between `academy` and `pixar_films` contain?
#    Try to find out without loading all the data into memory. Explain.

left_join(pixar_films, academy, join_by(film)) |>
  count()

count(academy)

# 2. Which films are not yet listed in the `academy` table? What does the
#    resulting SQL query look like?
#    - Hint: Use `anti_join()`

anti_join(pixar_films, academy, join_by(film))

# 3. Plot a bar chart with the number of awards won and nominated per year.
#    Compute as much as possible on the database.
#    - Hint: "Long form" or "wide form"?


academy_won_nominated <-
  academy |>
  filter(status %in% c("Nominated", "Won")) |>
  select(film, status)

per_year_won_nominated <-
  pixar_films |>
  transmute(film, year = year(release_date)) |>
  inner_join(academy_won_nominated, join_by(film)) |>
  count(year, status) |>
  collect()
per_year_won_nominated

ggplot(per_year_won_nominated, aes(x = year, y = n, fill = status)) +
  geom_col()

```

## Question 21

### 

```{r}
library(DBI)
library(tidyverse)
requireNamespace("duckplyr")
```


```{r}
### Working with database dumps #################################################

# Create data -------------------------------------------------------------------

arrow::write_parquet(nycflights13::flights, "flights.parquet")

fs::file_size("flights.parquet")
object.size(nycflights13::flights)

# Processing the local data ----

# Read as tibble ----

df <- arrow::read_parquet("flights.parquet")
df

# Read as Arrow dataset ----

ds <- arrow::open_dataset("flights.parquet")
ds
ds |>
  count(year, month, day) |>
  collect()
```


```{r}
# Register as duckdb lazy table ----

con_memory <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")

tbl <- duckdb::tbl_file(con_memory, "flights.parquet")
tbl
class(tbl)

tbl |>
  count(year, month, day)

tbl |>
  count(year, month, day) |>
  filter(month == 1) |>
  explain()
```


```{r}
# The future: Register as duckplyr lazy data frame ----

duckplyr_df <- duckplyr::duckplyr_df_from_parquet("flights.parquet")
class(duckplyr_df)

filtered <-
  duckplyr_df |>
  count(year, month, day) |>
  filter(month == 1)

filtered |>
  explain()

filtered

filtered |>
  explain()

duckplyr_df |>
  count(year, month, day) |>
  filter(month == 1L) |>
  explain()
```


```{r}
# Create partitioned data ------------------------------------------------------------------

arrow::write_dataset(
  nycflights13::flights,
  "flights-part/",
  partitioning = c("year", "month")
)

fs::dir_tree("flights-part")
```


```{r}
# Read partitioned data ------------------------------------------------------------------

tbl_part <- duckdb::tbl_query(
  con_memory,
  "read_parquet('flights-part/*/*/*.parquet', hive_partitioning = true)"
)
tbl_part
class(tbl_part)

tbl_part |>
  count(year, month, day)

tbl_part |>
  filter(month %in% 1:3) |>
  explain()
```


```{r}
# Create CSV data ------------------------------------------------------------------------

readr::write_csv(nycflights13::flights, "flights.csv")
```


```{r}
# Read CSV data --------------------------------------------------------------------------

tbl_csv <- duckdb::tbl_file(con_memory, "flights.csv")

tbl_csv |>
  count(year, month, day)

tbl_csv |>
  count(year, month, day) |>
  explain()

duckplyr_df_csv <- duckplyr::duckplyr_df_from_csv("flights.csv")

duckplyr_df_csv |>
  count(year, month, day)

duckplyr_df_csv |>
  count(year, month, day) |>
  explain()
```


```{r}
# Create derived Parquet data with duckplyr ---------------------------------------------------------

duckplyr_df_csv |>
  count(year, month, day) |>
  duckplyr::df_to_parquet("flights-count.parquet")

fs::file_size("flights-count.parquet")

duckplyr_df_count <-
  duckplyr::duckplyr_df_from_parquet("flights-count.parquet")

duckplyr_df_count |>
  explain()

duckplyr_df_count

duckplyr_df_count |>
  explain()
```


### Exercises

```{r}
# Exercises -------------------------------------------------------------------------

arrow::write_parquet(nycflights13::flights, "flights.parquet")

# 1. From the Parquet file, compute a lazy dbplyr tables
#    showing the mean and median departure delay
#    for each month.

con <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")

flights <- duckdb::tbl_file(con, "flights.parquet")

month_delay <-
  flights |>
  summarise(
    .by = month,
    mean_delay = mean(dep_delay),
    median_delay = median(dep_delay)
  )

month_delay

# 2. Compute the same data as duckplyr lazy data frames.

nycflights13::flights |>
  select(month, dep_delay) |>
  duckplyr::as_duckplyr_df() |>
  summarise(
    .by = month,
    mean_delay = mean(dep_delay),
    median_delay = median(dep_delay)
  )

# 3. Store this data as a Parquet file.

nycflights13::flights |>
  select(month, dep_delay) |>
  duckplyr::as_duckplyr_df() |>
  summarise(
    .by = month,
    mean_delay = mean(dep_delay),
    median_delay = median(dep_delay),
  ) |>
  duckplyr::df_to_parquet("delay-by-month.parquet")

# 4. Read the Parquet file and plot the data.

library(ggplot2)

duckplyr::duckplyr_df_from_parquet("delay-by-month.parquet") |>
  pivot_longer(cols = c(mean_delay, median_delay), names_to = "delay_type", values_to = "delay") |>
  ggplot(aes(x = month, y = delay, color = delay_type)) +
  geom_point() +
  geom_line() +
  labs(title = "Mean delay by month")


```


## Question 22

```{r}
library(DBI)
library(dplyr)

### DuckDB + SQL showcase #######################################################

# Create data -------------------------------------------------------------------

arrow::write_parquet(nycflights13::flights, "flights.parquet")
con_memory <- dbConnect(duckdb::duckdb(), dbdir = ":memory:")
tbl <- duckdb::tbl_file(con_memory, "flights.parquet")

# Application: DBI <=> dbplyr and pivoting -------------------------------------------------

daily_flights_by_dest <-
  tbl |>
  count(year, month, day, dest)

daily_flights_by_dest

daily_flights_by_dest_sql <-
  daily_flights_by_dest |>
  dbplyr::sql_render()
daily_flights_by_dest_sql

pivot_sql <- paste0(
  "PIVOT (", daily_flights_by_dest_sql, ") ON dest USING SUM(n)"
)

as_tibble(dbGetQuery(con_memory, pivot_sql))

system.time(
  as_tibble(dbGetQuery(con_memory, pivot_sql))
)

system.time(
  nycflights13::flights |>
    count(year, month, day, dest) |>
    tidyr::pivot_wider(names_from = dest, values_from = n, values_fill = 0)
)

write_pivot_sql <- paste0(
  "COPY (", pivot_sql, ") TO 'pivot.parquet' (FORMAT PARQUET)"
)
dbExecute(con_memory, write_pivot_sql)

q_unpivot_dyn <-
  "(SELECT * FROM (
   UNPIVOT 'pivot.parquet'
   ON COLUMNS(* EXCLUDE (year, month, day))
   INTO NAME dest VALUE n))"
tbl(con_memory, from = q_unpivot_dyn)

```



## Question 23

```{r}
library(DBI)
library(duckdb)
library(dplyr)
library(dbplyr)

### Database dumps #############################################################

# Connection -------------------------------------------------------------------

if (fs::file_exists("flights.duckdb")) {
  fs::file_delete("flights.duckdb")
}

con_rw <- dbConnect(duckdb::duckdb(), dbdir = "flights.duckdb")
flights_duckdb <- copy_to(
  con_rw,
  nycflights13::flights,
  name = "flights",
  temporary = FALSE
)
dbDisconnect(con_rw)

# Exploration ----

con <- dbConnect(
  duckdb::duckdb(),
  dbdir = "flights.duckdb",
  read_only = TRUE
)
flights_duckdb <- tbl(con, "flights")

# Method 1: via local data frame ----

flights_duckdb |>
  filter(month == 1) |>
  collect() |>
  duckplyr::df_to_parquet("flights-jan.parquet")

flights_duckdb |>
  collect() |>
  duckplyr::df_to_parquet("flights.parquet")

# Method 2: via DBI ----

sql_jan <- flights_duckdb |>
  filter(month == 1) |>
  dbplyr::sql_render()

fs::dir_create("flights-arrow")

res <- dbSendQuery(con, sql_jan)
i <- 0
repeat {
  df <- dbFetch(res, n = 10000)
  if (nrow(df) == 0) break
  path <- fs::path("flights-arrow", sprintf("part-%05d.parquet", i))
  duckplyr::df_to_parquet(df, path)
  i <- i + 1
  message("Written ", nrow(df), " rows to ", path)
}
dbClearResult(res)

fs::dir_tree("flights-arrow/")

# Method 3: via parquetize ----

parquetize::dbi_to_parquet(
  con,
  sql_jan,
  "flights-parquetized",
  max_rows = 10000
)

fs::dir_tree("flights-parquetized/")

# Method 4: via DBI and arrow ----



# con_adbi <- dbConnect(
#   adbi::adbi(duckdb::duckdb_adbc()),
#   path = "flights.duckdb"
# )
# 
# sql <- "SELECT * FROM flights"
# 
# res <- dbSendQueryArrow(con_adbi, sql)
# stream <- dbFetchArrow(res)
# arrow::write_dataset(
#   arrow::as_record_batch_reader(stream),
#   "flights-adbi/"
# )
# dbClearResult(res)

# Partitions ----

nycflights13::flights |>
  arrow::write_dataset(
    "flights-part-arrow/",
    partitioning = "month"
  )

fs::dir_tree("flights-part-arrow/")

# Adding partitions to a dataset ----

write_month <- function(month) {
  sql <- flights_duckdb |>
    filter(month == !!month) |>
    dbplyr::sql_render()

  dir <- fs::path(
    "flights-part-manual",
    sprintf("month=%d", month)
  )
  fs::dir_create(dir)

  df <- dbGetQuery(con, sql)
  duckplyr::df_to_parquet(
    df,
    fs::path(dir, "part-0.parquet")
  )
}

write_month(1)
write_month(2)
write_month(3)

fs::dir_tree("flights-part-manual")

# Exercises -------------------------------------------------------------------------



# 1. Write code to create a partitioned dataset with the `flights` table,
#    partitioned by `origin`.
#        - Hint: The dataset only contains flights departing from New York City airports.

```


## Question 31

### 

```{r}
# attach relevant packages
library(tidyverse)
library(DBI)

### Extract, Transform, Load ###################################################

# Extract: Raw data ------------------------------------------------------------

pixar_films_raw <- pixarfilms::pixar_films
pixar_films_raw

# Transform: Fix column type, extract sequel column ----------------------------

pixar_films_clean <-
  pixar_films_raw |>
  separate(film, into = c("franchise", "sequel"),
    sep = " (?=[0-9]+$)", fill = "right", remove = FALSE
  ) |>
  mutate(across(c(number, sequel), as.integer)) |>
  mutate(.by = franchise, sequel = if_else(is.na(sequel) & n() > 1, 1L, sequel))
pixar_films_clean

# Create target database -------------------------------------------------------

if (fs::file_exists("pixar.duckdb")) {
  fs::file_delete("pixar.duckdb")
}

# Load: Write table to the database --------------------------------------------

con_rw <- dbConnect(duckdb::duckdb(), dbdir = "pixar.duckdb")
con_rw

if (!dbExistsTable(con_rw, "pixar_films")) {
  dbWriteTable(con_rw, "pixar_films", pixar_films_clean)
  dbExecute(con_rw, "CREATE UNIQUE INDEX pixarfilms_pk ON pixar_films (film)")
}

dbDisconnect(con_rw)

# Reload: Write table to the database if the table exists ----------------------------------

con_rw <- dbConnect(duckdb::duckdb(), dbdir = "pixar.duckdb")
con_rw

dbExecute(con_rw, "TRUNCATE TABLE pixar_films")
dbAppendTable(con_rw, "pixar_films", pixar_films_clean)

dbDisconnect(con_rw)

# Consume: share the file, open it ---------------------------------------------

con <- dbConnect(duckdb::duckdb(), dbdir = "pixar.duckdb")
my_pixar_films <- tbl(con, "pixar_films")
my_pixar_films
```


```{r}
# Exercises --------------------------------------------------------------------

# 1. Adapt the ETL workflow to convert the `run_time` column to a duration.

pixar_films_clean <-
  pixar_films_clean |>
  mutate(run_time = hms::hms(minutes = run_time))
pixar_films_clean

#    - Hint: Use `mutate()` with `hms::hms(minutes = ...)` .
# 2. Re-run the workflow.


```

## Question 32

```{r}
# attach relevant packages
library(DBI)
library(dm)

### Remote databases ###################################################

# Connect --------------------------------------------------------------

con <- dbConnect(
  RMariaDB::MariaDB(),
  dbname = "CORA",
  username = "guest",
  password = "ctu-relational",
  host = "relational.fel.cvut.cz"
)

# List tables ----------------------------------------------------------

dbListTables(con)

# Use dm for many tables -----------------------------------------------

dm <- dm_from_con(con)

dm

dm |>
  dm_nrow()

dm$paper

dm |>
  dm_get_tables()

```




## Question 33

### Questions

```{r}
# attach relevant packages
library(tidyverse)
library(dm)

# display chosen presentation (it might take a few seconds to appear)
slide_viewer <- function(path) {
  tmp <- tempfile(fileext = ".html")
  file.copy(path, tmp)
  rstudioapi::viewer(tmp)
}
# slide_viewer("materials/databases.html")

### Data models ################################################################

# Data model objects -----

pixar_dm <- dm_pixarfilms()
pixar_dm

pixar_dm |>
  dm_draw()

names(pixar_dm)

pixar_dm$pixar_films
pixar_dm$academy

pixar_dm |>
  dm_get_tables()

# Showcase: wrapping all tables in a data model:
pixar_films_wrapped <-
  pixar_dm |>
  dm_wrap_tbl(pixar_films) |>
  pull_tbl(pixar_films)

pixar_films_wrapped
pixar_films_wrapped$academy[1:2]


### Keys, constraints, normalization ###########################################

# Data model object ------

pixar_dm <- dm_pixarfilms()

# Primary keys ----

any(duplicated(pixar_dm$pixar_films$film))
check_key(pixar_dm$pixar_films, film)
any(duplicated(pixar_dm$academy[c("film", "award_type")]))
check_key(pixar_dm$academy, film, award_type)
try(
  check_key(pixar_dm$academy, film)
)

# Foreign keys ----

all(pixar_dm$academy$film %in% pixar_dm$pixar_films$film)
check_subset(pixar_dm$academy, film, pixar_dm$pixar_films, film)
try(
  check_subset(pixar_dm$pixar_films, film, pixar_dm$academy, film)
)

# Constraints ----

pixar_dm |>
  dm_examine_constraints()

dm_pixarfilms(consistent = TRUE) |>
  dm_examine_constraints()

dm_nycflights13() |>
  dm_examine_constraints()

# Joins ----

pixar_dm |>
  dm_zoom_to(academy)

# With zooming:
pixar_dm |>
  dm_zoom_to(academy) |>
  left_join(pixar_films, select = c(film, release_date))

# With flattening:
pixar_dm |>
  dm_flatten_to_tbl(academy)

dm_nycflights13() |>
  dm_select(weather, -year, -month, -day, -hour) |>
  dm_flatten_to_tbl(flights)

# Joining is easy, leave the tables separate for as long as possible!

# Exercises --------------------------------------------------------------------

venue <- tibble(
  venue_id = character(),
  floor = character(),
  capacity = integer(),
)

event <- tibble(
  event_id = character(),
  event_name = character(),
  event_type = character(),
  venue_id = character(),
  date_start = vctrs::new_datetime(),
  date_end = vctrs::new_datetime(),
)

attendee <- tibble(
  attendee_name = character(),
  favorite_package = character(),
)

speaker <- tibble(
  speaker_name = character(),
  event_id = character(),
)

event_attendee <- tibble(
  event_id = character(),
  attendee_name = character(),
)

```



### Solutions

```{r}
library(tibble)
library(dm)
library(DBI)

# 1. Explore <https://dm.cynkra.com> and the built-in data models
#     `dm_nycflights13()` and `dm_pixarfilms()`

dm_nycflights13() |>
  dm_draw()

dm_pixarfilms() |>
  dm_draw(view_type = "all")

# 2.

venue <- tibble(
  venue_id = character(),
  floor = character(),
  capacity = integer(),
)

event <- tibble(
  event_id = character(),
  event_name = character(),
  event_type = character(),
  venue_id = character(),
  date_start = vctrs::new_datetime(),
  date_end = vctrs::new_datetime(),
)

attendee <- tibble(
  attendee_name = character(),
  favorite_package = character(),
)

speaker <- tibble(
  speaker_name = character(),
  event_id = character(),
)

event_attendee <- tibble(
  event_id = character(),
  attendee_name = character(),
)

# 2. Given the table structure above, create a dm object setting suitable
#     PK and FK relationships and unique keys.
#     Each speaker is an attendee, each event has a venue and exactly one speaker.
#     The helper table event_attendees matches attendees to events.
#     - Hint: Use the `dm()` function to create a dm object from scratch
#     - Hint: Use a unique key on `speakers$event_name`
dm_conf_target <-
  dm(venue, event, attendee, speaker, event_attendee) |>
  dm_add_pk(venue, venue_id) |>
  dm_add_pk(event, event_id) |>
  dm_add_pk(speaker, speaker_name) |>
  dm_add_pk(attendee, attendee_name) |>
  dm_add_fk(speaker, event_id, event) |>
  dm_add_fk(event, venue_id, venue) |>
  dm_add_fk(speaker, speaker_name, attendee, attendee_name) |>
  dm_add_fk(event_attendee, event_id, event) |>
  dm_add_fk(event_attendee, attendee_name, attendee) |>
  dm_add_uk(speaker, event_id)

# 3. Draw the dm object
dm_conf_target |>
  dm_draw()

# 4. Color the tables (optional)
dm_conf_target |>
  dm_set_colors(
    blue = event,
    red = venue,
    green3 = speaker,
    seagreen = attendee,
  ) |>
  dm_draw()

# 5. Deploy the data model to a DuckDB database
con_rw <- dbConnect(duckdb::duckdb(), "posit-conf.duckdb")
dm_conf_target <- copy_dm_to(con_rw, dm_conf_target, temporary = FALSE)

dbListTables(con_rw)

dm_conf_target |>
  dm_get_tables()

dbDisconnect(con_rw)

```


## Question 40 (backup)

###

```{r}
library(DBI)
library(duckdb)
library(dplyr)
library(dm)

fs::dir_create("sec")

# https://www.sec.gov/files/structureddata/data/form-d-data-sets/2023q4_d.zip

sec_paths <- fs::dir_ls("sec")

if (FALSE) {
  purrr::walk(sec_paths, ~ unzip(.x, exdir = "sec-unzipped"))
}

if (fs::file_exists("formd.duckdb")) {
  fs::file_delete("formd.duckdb")
}

# Form D ------------------------------------------------------------------------

duckdb_con <- dbConnect(duckdb())

form_d <- duckdb::tbl_file(
  duckdb_con,
  "sec-unzipped/2023Q4_d/FORMDSUBMISSION.tsv"
)

# explore column names
try(names(form_d))
colnames(form_d) |>
  writeLines()

# duplicates check
form_d |>
  rename_with(tolower) |>
  summarise(.by = accessionnumber, n = n()) |>
  filter(n > 1) |>
  count()

# Issuers ----------------------------------------------------------------------

issuers <- duckdb::tbl_file(
  duckdb_con,
  "sec-unzipped/2023Q4_d/ISSUERS.tsv"
)

# explore column names
colnames(issuers) |>
  writeLines()

# duplicates check
issuers |>
  rename_with(tolower) |>
  summarise(.by = accessionnumber, n = n()) |>
  filter(n > 1) |>
  count()

issuers |>
  rename_with(tolower) |>
  summarise(.by = c(accessionnumber, issuer_seq_key), n = n()) |>
  filter(n > 1) |>
  count()


# Offering ---------------------------------------------------------------------

offering <- duckdb::tbl_file(
  duckdb_con,
  "sec-unzipped/2023Q4_d/OFFERING.tsv"
)

# explore column names
colnames(offering) |>
  writeLines()

# duplicates check
offering |>
  rename_with(tolower) |>
  summarise(.by = accessionnumber, n = n()) |>
  filter(n > 1) |>
  count()

# Recipients -------------------------------------------------------------------

recipients <- duckdb::tbl_file(
  duckdb_con,
  "sec-unzipped/2023Q4_d/RECIPIENTS.tsv"
)

# explore column names
colnames(recipients) |>
  writeLines()

recipients |>
  rename_with(tolower) |>
  summarise(.by = accessionnumber, n = n()) |>
  filter(n > 1) |>
  count()

recipients |>
  rename_with(tolower) |>
  summarise(.by = c(accessionnumber, recipient_seq_key), n = n()) |>
  filter(n > 1) |>
  count()

# dm ---------------------------------------------------------------------------

dm_formd_set_pk_fk <- function(dm) {

  stopifnot(is_dm(dm))

  dm |>
    dm_add_pk(form_d, ACCESSIONNUMBER, check = TRUE) |>
    dm_add_pk(issuers, c(ACCESSIONNUMBER, ISSUER_SEQ_KEY)) |>
    dm_add_pk(offering, ACCESSIONNUMBER) |>
    dm_add_pk(recipients, c(ACCESSIONNUMBER, RECIPIENT_SEQ_KEY)) |>
    dm_add_fk(issuers, ACCESSIONNUMBER, form_d) |>
    dm_add_fk(offering, ACCESSIONNUMBER, form_d) |>
    dm_add_fk(recipients, ACCESSIONNUMBER, form_d)

}

formd_dm_keys <-
  dm(form_d, issuers, offering, recipients) |>
  dm_formd_set_pk_fk()

dm_draw(formd_dm_keys)

dm_examine_constraints(formd_dm_keys)

# Analyze ----------------------------------------------------------------------

base_dat <-
  formd_dm_keys |>
  dm_flatten_to_tbl(.start = issuers) |> # help(dm_flatten_to_tbl)
  rename_with(tolower) |>
  left_join(
    rename_with(pull_tbl(formd_dm_keys, offering), tolower),
    join_by(accessionnumber)
  ) |>
  mutate(
    filing_date = sql("STRPTIME(filing_date, '%d-%b-%Y')")
  ) |>
  mutate(
    filing_date = sql("CAST(filing_date AS DATE)")
  ) |>
  transmute(
    year = lubridate::year(filing_date),
    month = lubridate::month(filing_date),
    accessionnumber,
    entityname,
    stateorcountry,
    stateorcountrydescription,
    entitytype,
    federalexemptions_items_list,
    submissiontype,
    totalamountsold,
    totalofferingamount = as.numeric(
      sql("nullif(totalofferingamount, 'Indefinite')")
    )
  )

# submissiontype per month ----

type_dat <-
  base_dat |>
  count(year, month, submissiontype) |>
  collect() |>
  mutate(filing_date = lubridate::make_date(year, month)) |>
  arrange(year, month)

library(ggplot2)
type_dat |>
  mutate(dte = lubridate::make_date(year, month)) |>
  ggplot(aes(dte, n, fill = submissiontype)) +
  geom_col(position = "dodge") +
  theme_minimal() +
  labs(title = "Form D submission Q1")

# amount sold per state ----
base_dat |>
  summarise(
    .by = stateorcountrydescription,
    tot_sold = sum(totalamountsold, na.rm = TRUE),
    tot_offered = sum(totalofferingamount, na.rm = TRUE)
  ) |>
  filter(tot_sold > 0) |>
  collect() |>
  mutate(
    stateorcountrydescription = forcats::fct_reorder(
      stateorcountrydescription,
      tot_sold
    )
  ) |>
  ggplot(aes(stateorcountrydescription, tot_sold)) +
  geom_col() +
  coord_flip() +
  theme_minimal()


# rank ten best raising capital ----
base_dat |>
  dbplyr::window_order(totalamountsold) |>
  mutate(row_num = row_number()) |>
  filter(between(row_num, max(row_num) - 9L, max(row_num))) |>
  collect() |>
  arrange(desc(totalamountsold)) |>
  select(entityname, totalamountsold)

# Multi ------------------------------------------------------------------------

q_form_d <-
  "CREATE OR REPLACE TABLE form_d AS
   SELECT *
   FROM read_csv(
          'sec-unzipped/*/FORMDSUBMISSION.tsv',
          types={'FILING_DATE': 'VARCHAR'}
        );
  "

dbExecute(duckdb_con, q_form_d)

q_issuers <-
  "CREATE OR REPLACE TABLE issuers AS
   SELECT *
   FROM read_csv(
          'sec-unzipped/*/ISSUERS.tsv'
        );
  "

dbExecute(duckdb_con, q_issuers)

q_offering <-
  "CREATE OR REPLACE TABLE offering AS
   SELECT *
   FROM read_csv('sec-unzipped/*/OFFERING.tsv');
  "

dbExecute(duckdb_con, q_offering)

q_recipients <-
  "CREATE OR REPLACE TABLE recipients AS
   SELECT *
   FROM read_csv('sec-unzipped/*/RECIPIENTS.tsv');
  "

dbExecute(duckdb_con, q_recipients)

stopifnot(length(dbListTables(duckdb_con)) == 4L)

formd_dm <-
  dm::dm_from_con(duckdb_con) |>
  dm_formd_set_pk_fk()

dbDisconnect(duckdb_con)

```





